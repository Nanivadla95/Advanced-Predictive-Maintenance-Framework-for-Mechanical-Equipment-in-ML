{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyM2kxbT9zeNT8Ze0D52NVFC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nanivadla95/Advanced-Predictive-Maintenance-Framework-for-Mechanical-Equipment-in-ML/blob/main/LLM_Fine_Tuning_Comparison_Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuS2xtpdMD1U"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers datasets evaluate -q\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "zM7dEEHuMLW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import evaluate\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "Fkfzr1qWNBmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing the Base LLM"
      ],
      "metadata": {
        "id": "2Jxd5gcmNQXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_to_compare = [\"bert-base-uncased\", \"roberta-base\"]\n"
      ],
      "metadata": {
        "id": "cGqcgPqjN1vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Task and Data Preparation\n",
        "\n",
        "\n",
        "\n",
        "https://huggingface.co/settings/tokens\n"
      ],
      "metadata": {
        "id": "Zh0xfSsoNzMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Access the 'train' split, then shuffle and select\n",
        "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(5000))\n",
        "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n"
      ],
      "metadata": {
        "id": "JMqljJebOHsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization function\n"
      ],
      "metadata": {
        "id": "5y1MHfJqYIy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n"
      ],
      "metadata": {
        "id": "UQOADGQkYcbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " #  Evaluation Metric"
      ],
      "metadata": {
        "id": "6rSewEXvYmJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n"
      ],
      "metadata": {
        "id": "vlIe5sP1YtW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Fine-Tuning Process\n",
        "We loop over both models, tokenize, fine-tune with Trainer API, and evaluate."
      ],
      "metadata": {
        "id": "fVnkbijzYy9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "for model_name in models_to_compare:\n",
        "    print(f\"\\nðŸš€ Fine-tuning {model_name}...\\n\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{model_name}\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=1,\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\",\n",
        "        logging_steps=10\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset[\"train\"].select(range(4000)),\n",
        "        eval_dataset=tokenized_dataset[\"train\"].select(range(4000, 5000)),\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    results[model_name] = eval_result[\"eval_accuracy\"]\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VA4IdWitX_dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Evaluation and Performance"
      ],
      "metadata": {
        "id": "53KuvoYSZAYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nðŸ“Š LLM Fine-Tuning Results Comparison:\")\n",
        "for model, acc in results.items():\n",
        "    print(f\"{model}: Accuracy = {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "fZfeKjghYXwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "cTOG1-r2ZKw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "GL0FTj1PZTR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# Load and subset the dataset\n",
        "raw_dataset = load_dataset(\"imdb\")\n",
        "train_dataset = raw_dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "test_dataset = raw_dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
        "\n",
        "# Load evaluation metric\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "# List of base models to compare\n",
        "models_to_compare = [\"bert-base-uncased\", \"roberta-base\"]\n",
        "\n",
        "for model_name in models_to_compare:\n",
        "    print(f\"\\nðŸš€ Fine-tuning {model_name}...\\n\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Define tokenize function inside the loop\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # Tokenize the dataset\n",
        "    tokenized_train = train_dataset.map(tokenize, batched=True)\n",
        "    tokenized_test = test_dataset.map(tokenize, batched=True)\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    # Set training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{model_name}\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=1,\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    # Accuracy metric\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # Set up Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_test,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Fine-tune\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    print(f\"ðŸ“Š Accuracy for {model_name}: {eval_result['eval_accuracy']:.4f}\")\n"
      ],
      "metadata": {
        "id": "KpXLCPEAemKA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}